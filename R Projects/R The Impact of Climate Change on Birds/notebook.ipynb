{"nbformat_minor":2,"cells":[{"metadata":{"dc":{"key":"3"},"editable":false,"run_control":{"frozen":true},"deletable":false,"tags":["context"]},"cell_type":"markdown","source":"## 1. Tracking a changing climate\n<p>The climate is changing around the world. The impacts of climate change are felt in many different areas, but they are particularly noticeable in their effects on birds. Many bird species are moving north, if they can,  to stay in climatic conditions that are suitable for them.</p>\n<p>Our analysis will use data from the <a href=\"https://www.metoffice.gov.uk/climate/uk/data/ukcp09\">UK Met Office</a> together with records from the <a href=\"https://www.gbif.org/\">Global Biodiversity Information Facility</a> to build our very own species distribution model using machine learning. This model will be able to predict where our bird species of interest is likely to occur in the future - information that is invaluable to conservation organization working on the ground to preserve these species and save them from extinction!</p>\n<p>In this notebook, we will model the Scottish crossbill (<em>Loxia scotica</em>). The Scottish crossbills is a small bird that inhabits the cool Scottish forests and feeds on pine seeds. Only ~ 20,000 individuals of this species are alive today. The code and the data sources in this project can be reapplied to any other species we may be interested in studying.</p>\n<p><img src=\"https://assets.datacamp.com/production/project_664/img/Loxia.jpg\" alt=\"Loxia.jpg\"></p>\n<p>We will start by importing the climate data from a local <code>rds</code> file.</p>"},{"execution_count":null,"metadata":{"dc":{"key":"3"},"trusted":true,"tags":["sample_code"]},"outputs":[],"cell_type":"code","source":"# Load in the tidyverse, raster, and sf packages\nlibrary('tidyverse')\nlibrary('sf')\nlibrary('raster')\n# Read the climate data from an rds file\nclimate  <- read_rds('datasets/climate_raster.rds')\n\n# Have a look at the variables in the climate data\ncolnames(climate)\n\n# Convert to SpatialPixelDataFrame for plotting\nclimate_df <- mutate(\n  .data = climate, \n  rasters = map(\n    .x = rasters, \n    ~ as_tibble(as(.x, \"SpatialPixelsDataFrame\")))) %>%\n  unnest(cols = c(rasters))"},{"metadata":{"dc":{"key":"10"},"editable":false,"run_control":{"frozen":true},"deletable":false,"tags":["context"]},"cell_type":"markdown","source":"## 2. Mapping a changing climate\n<p>We have loaded the pre-processed climate data and converted it to a <code>SpatialPixelDataFrame</code>. This data frame now contains all the information we need:</p>\n<ul>\n<li>the <code>decade</code> of observation,</li>\n<li>spatial coordinates (<code>x</code>, <code>y</code>)</li>\n<li>six selected climatic variables (<code>minimum.temperature</code>, <code>maximum.temperature</code>, <code>rainfall</code>, <code>wind.speed</code>, <code>snow.lying</code>, <code>air.frost</code>)</li>\n</ul>\n<p>An excellent first step in any analysis is visualizing the data. Visualizing the data makes sure the data import worked, and it helps us develop intuition about the patterns in our dataset. Here we are dealing with spatial data - let us create maps! We will start with two maps: one map of the climatic conditions in 1970, and one map of the climatic conditions in 2010. Our climate data has several variables, so let us pick <code>minimum.temperature</code> for now.</p>"},{"execution_count":null,"metadata":{"dc":{"key":"10"},"trusted":true,"tags":["sample_code"]},"outputs":[],"cell_type":"code","source":"library(ggthemes)\n\n# Filter the data to plot\nggp_temperature <- climate_df %>%\n  filter(decade %in% 1970:2010)%>%\n  # Create the plot\n  ggplot(aes(x = x, y = y)) + geom_tile(aes(fill = minimum.temperature)) +\n  # Style the plot with options ideal for maps\n  theme_map()+\n  coord_equal()+\n  facet_grid(~ decade) + scale_fill_distiller(palette = \"Spectral\") + \n  theme(legend.title = element_blank(), legend.position = \"bottom\") +\n  labs(title = \"Minimum of Average Monthly Temperature (Celsius)\", caption = 'Source: MetOffice UK')\n\n# Display the map\nggp_temperature"},{"metadata":{"dc":{"key":"17"},"editable":false,"run_control":{"frozen":true},"deletable":false,"tags":["context"]},"cell_type":"markdown","source":"## 3. Fieldwork in the digital age – download the data\n<p>Now we need to obtain species occurrence records. This used to be the main challenge in biogeography. Natural historians, such as Charles Darwin and Alexander von Humboldt, traveled around the globe for years on rustic sail ships collecting animal and plant specimens to understand the natural world. Today, we stand on the shoulders of giants. Getting data is fast and easy thanks to two organizations:</p>\n<ul>\n<li><p><a href=\"https://www.gbif.org/\">The Global Biodiversity Information Facility (GBIF)</a>, an international network and research infrastructure aimed at providing anyone, anywhere, open access to data about life on Earth. We will use their data in this project.</p></li>\n<li><p><a href=\"https://ropensci.org/\">rOpenSci</a>, a non-profit initiative that develops open source tools for academic data sets. Their package <code>rgbif</code> will help us access the species data.</p></li>\n</ul>"},{"execution_count":null,"metadata":{"dc":{"key":"17"},"trusted":true,"tags":["sample_code"]},"outputs":[],"cell_type":"code","source":"library(rgbif)\nsource(\"datasets/occ_search.R\")\n\n# Call the API to get the occurrence records of this species\ngbif_response <- occ_search(\n  scientificName = \"Loxia scotica\", country = \"GB\",\n  hasCoordinate = TRUE, hasGeospatialIssue = FALSE, limit = 2000)\n\n# Inspect the class and names of gbif_response\nclass(gbif_response)\nnames(gbif_response)\n# Print the first six lines of the data element in gbif_response\nhead(gbif_response$data)"},{"metadata":{"dc":{"key":"24"},"editable":false,"run_control":{"frozen":true},"deletable":false,"tags":["context"]},"cell_type":"markdown","source":"## 4. Sorting out the bad eggs – data cleaning\n<p>GBIF and rOpenSci just saved us years of roaming around the highlands with a pair of binoculars, camping in mud, rain, and snow, and chasing crossbills through the forest! Nevertheless, it is still up to us to make sense of the data. In particular, data collected at this large scale can have issues. Luckily, GBIF provides some useful metadata on each record.</p>\n<p>Here are some criteria we can use: </p>\n<ol>\n<li>\"issues\" - We will only use records where no doubts about the observation were listed.</li>\n<li>\"license\" - We will only use records under a creative commons license.</li>\n<li>\"date\" - We will only use records between 1965 and 2015 because that matches our climate dataset.</li>\n</ol>"},{"execution_count":null,"metadata":{"dc":{"key":"24"},"trusted":true,"tags":["sample_code"]},"outputs":[],"cell_type":"code","source":"library(lubridate)\n\nbirds_dated <- mutate(\n  .data = gbif_response$data,\n  # Create a new column specifying the decade of observation\n  decade = ymd_hms(eventDate) %>% round_date(\"10y\") %>% year())\n\nbirds_cleaned <- birds_dated %>%\n  filter(\n    issues == \"\" &\n    str_detect(license, \"http://creativecommons.org/\") &\n    # No records before 1970s decade or after 2010s decade\n    decade %in% 1970:2010\n  ) %>%\n  transmute(decade = decade, x = decimalLongitude, y = decimalLatitude) %>%\n  arrange(decade)"},{"metadata":{"dc":{"key":"31"},"editable":false,"run_control":{"frozen":true},"deletable":false,"tags":["context"]},"cell_type":"markdown","source":"## 5. Nesting the data\n<p>We have cleaned the data, but there is a problem. We want to know the climatic conditions at the location of the bird observation <strong>at the time</strong> of the observation. This is tricky because we have climate data from multiple decades. How do we match each bird observation to the correct climate raster cell?</p>\n<p>We will use a nifty trick: we can <code>nest()</code> data in a list column. The result is a data frame where the grouping columns do not change, and a list column of aggregated data from each group is added. List columns can hold many different kinds of variables such as vectors, data frames, and even objects. For example, the climate data that we imported earlier was already nested by decade and had a list column (<code>rasters</code>) that contained a <code>rasterStack</code> object for each decade.</p>"},{"execution_count":null,"metadata":{"dc":{"key":"31"},"trusted":true,"tags":["sample_code"]},"outputs":[],"cell_type":"code","source":"# \"Nest\" the bird data\nbirds_nested <- birds_cleaned%>%\n    group_by(decade)%>%\n    nest(.key='presences')\n\nhead(birds_nested)\n\n# Calculate the total number of records per decade\nbirds_counted <- birds_nested %>%\n  mutate(n = map_dbl(.x=presences, .f=nrow))\n\nhead(birds_counted)"},{"metadata":{"dc":{"key":"39"},"editable":false,"run_control":{"frozen":true},"deletable":false,"tags":["context"]},"cell_type":"markdown","source":"## 6. Making things spatial - projecting our observations\n<p>Excellent! Both our datasets are nested by decade now. We have one more step before we extract the climatic conditions at bird locations. Locations in <code>birds_counted</code> are latitude and longitude coordinates. R doesn't know that these are spatial information. We need to <strong>convert</strong> and <strong>project</strong> our data.</p>\n<p>Projections are necessary because maps are 2-dimensional, but the earth is 3-dimensional. There is no entirely accurate way to represent the surface of a 3D sphere in 2D. Projections are sets of conventions to help us with this issue. GBIF hosts data from around the world and uses a global projection (WGS84). The Met Office is a UK organization and provides data in the British National Grid projection (BNG).</p>\n<p>To project spatial data, use Coordinate Reference System (CRS) strings.</p>"},{"execution_count":null,"metadata":{"dc":{"key":"39"},"trusted":true,"tags":["sample_code"]},"outputs":[],"cell_type":"code","source":"# Define geographical projections\nproj_latlon <- st_crs(\"+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0\")\nproj_ukgrid <- st_crs(\"+init=epsg:27700\")\n\n# Convert records to spatial points and project them\nbirds_presences <- mutate(birds_counted,\n  presences = map(presences, ~ .x %>%\n    # Specify the current projection\n    st_as_sf(coords = c(\"x\", \"y\"), crs = proj_latlon) %>%\n    # Transform to new projection\n    st_transform(crs = proj_ukgrid)))"},{"metadata":{"dc":{"key":"46"},"editable":false,"run_control":{"frozen":true},"deletable":false,"tags":["context"]},"cell_type":"markdown","source":"## 7. Extract exactly what we want\n<p>Now we are ready to combine the two datasets and extract the climatic conditions at each location for the given decade. This is where the nested structure comes in handy! We join the data frames by their grouping column and can rest assured that the data in the list columns are matched up correctly. This allows us to operate on the list column variables element-wise using the <code>map()</code> family functions.</p>"},{"execution_count":null,"metadata":{"dc":{"key":"46"},"trusted":true,"tags":["sample_code"]},"outputs":[],"cell_type":"code","source":"# Combine the bird data and the climate data in one data frame\nbirds_climate <- full_join(birds_presences, climate, by = \"decade\")\n\npresence_data <- map2_df(\n  .x = birds_climate[[\"rasters\"]],\n  .y = birds_climate[[\"presences\"]],\n  # extract the raster values at presence locations\n  ~ raster::extract(x=.x,y=.y) %>% \n    as_tibble() %>% \n    mutate(observation = \"presence\"))"},{"metadata":{"dc":{"key":"53"},"editable":false,"run_control":{"frozen":true},"deletable":false,"tags":["context"]},"cell_type":"markdown","source":"## 8. Pseudo-absences\n<p>To run a machine learning model, the classification algorithm needs two classes: presences and absences. Our presences are the observations from GBIF. Absences are a lot harder to get.</p>\n<p>The difficulty is because of information asymmetry between the presences and absences. With a bird observation we are sure it occurred at that location, but to be certain the bird does <strong>not</strong> occur somewhere, we would have to continuously monitor the site.</p>\n<p>One way to deal with this problem is to generate \"pseudo-absences\". Pseudo-absences are a random sample from the entire study area. We assume that the species does not occur at the random locations and our hope is that the average actual probability of occurrence for the bird in these random locations is low enough to give our algorithm something to learn</p>"},{"execution_count":null,"metadata":{"dc":{"key":"53"},"trusted":true,"collapsed":true,"tags":["sample_code"]},"outputs":[],"cell_type":"code","source":"# Define helper function for creating pseudo-absence data\ncreate_pseudo_absences <- function(rasters, n, ...) {\n    set.seed(12345)\n    sampleRandom(rasters, size = n * 5, sp = TRUE) %>% \n    raster::extract(rasters, .) %>% as_tibble() %>%\n    mutate(observation = \"pseudo_absence\")\n}\n\n# Create pseudo-absence proportional to the total number of records per decade\npseudo_absence_data <- pmap_df(.l = birds_climate, .f = create_pseudo_absences)\n\n# Combine the two datasets\nmodel_data <- bind_rows(presence_data, pseudo_absence_data) %>%\n  mutate(observation = factor(observation)) %>% na.omit()"},{"metadata":{"dc":{"key":"60"},"editable":false,"run_control":{"frozen":true},"deletable":false,"tags":["context"]},"cell_type":"markdown","source":"## 9. Making models - with caret\n<p>We are ready to train our model. We will use <code>glmnet</code>,  which fits a generalized logistic regression (<em>glm</em>) with elastic net regularization (<em>net</em>). Our algorithm has several \"hyperparameters\". These are variables used by the machine learning algorithm to learn from the data. They influence the performance of the model and often interact with one another, so it is difficult to know the right settings <em>apriori</em>.</p>\n<p>To figure out a good set of hyperparameters, we need to try several possible scenarios to see which ones work best. <code>caret</code> makes this easy. All we need to do is define a \"tuning grid\" with sets of possible values for each training parameter. Then use cross-validation to evaluate how well the different combinations of hyperparameters did building the predictive model.</p>"},{"execution_count":null,"metadata":{"dc":{"key":"60"},"trusted":true,"tags":["sample_code"]},"outputs":[],"cell_type":"code","source":"# Load caret and set a reproducible seed\nlibrary(caret)\nset.seed(12345)\n\n# Create a tuning grid with sets of hyperparameters to try\ntuneGrid <- expand.grid(alpha = c(0, 0.5, 1), lambda = c(.003, .01, .03, .06))\n\n# Create settings for model training\ntrControl <- trainControl(method = 'repeatedcv', number = 5, repeats = 1,\n  classProbs = TRUE, verboseIter = FALSE, summaryFunction = twoClassSummary)\n\n# Fit a statistical model to the data and plot\nmodel_fit <- train(\n  observation ~ ., data = model_data,\n  method = \"glmnet\", family = \"binomial\", metric = \"ROC\",\n  tuneGrid = tuneGrid, trControl = trControl)\n\nplot(model_fit)"},{"metadata":{"dc":{"key":"67"},"editable":false,"run_control":{"frozen":true},"deletable":false,"tags":["context"]},"cell_type":"markdown","source":"## 10. Prediction probabilities\n<p>Congratulations, we have built our first species distribution model! Next, we will use it to predict the probability of occurrence for our little crossbill across the UK! We will make a prediction for each decade and each cell of the grid. Since we fit a logistic regression model, we can choose to predict the probability. In our case, this becomes the \"probability of occurrence\" for our species.</p>"},{"execution_count":null,"metadata":{"dc":{"key":"67"},"trusted":true,"tags":["sample_code"]},"outputs":[],"cell_type":"code","source":"# Use our model to make a prediction\nclimate_df[[\"prediction\"]] <- predict(\n    object = model_fit,\n    newdata = climate_df,\n    type = \"prob\")[[\"presence\"]]\n\nhead(climate_df)"},{"metadata":{"dc":{"key":"74"},"editable":false,"run_control":{"frozen":true},"deletable":false,"tags":["context"]},"cell_type":"markdown","source":"## 11. A map says more than a thousand words\n<p>We have our predictions, but they are not in a digestible format. It is tough to figure out what is going on from that large table of numbers, and it would be even more challenging to convince a politician, the general public, or a local decision maker with it.</p>\n<p>It would be great to visualize the predictions so we can see the patterns and how they change over time. A picture says more than a thousand words. And what says even more than a picture (at least if you are a geographer)? A colored map! Let us create another map that shows our predictions of a changing climate in the UK, from 1965 to 2015.</p>"},{"execution_count":null,"metadata":{"dc":{"key":"74"},"trusted":true,"tags":["sample_code"]},"outputs":[],"cell_type":"code","source":"library(viridis)\n\n# Create the plot\nggp_changemap <- ggplot(climate_df,aes(x=x,y=y))+\n  geom_tile(aes(fill = prediction)) +\n  # Style the plot with the appropriate settings for a map\n  theme_map()+\n  coord_equal()+\n  scale_fill_viridis(option = \"A\") + theme(legend.position = \"bottom\") +\n  # Add faceting by decade\n  facet_grid(~decade)+\n  labs(title = 'Habitat Suitability', subtitle = 'by decade',\n       caption = 'Source:\\nGBIF data and\\nMetOffice UK climate data',\n       fill = 'Habitat Suitability [0 low - high 1]')\n\n# Display the plot\nggp_changemap"}],"metadata":{"kernelspec":{"display_name":"R","name":"ir","language":"R"},"language_info":{"pygments_lexer":"r","file_extension":".r","codemirror_mode":"r","mimetype":"text/x-r-source","name":"R","version":"3.4.1"}},"nbformat":4}